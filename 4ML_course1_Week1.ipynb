{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ML Specialization (COURSERA)"
      ],
      "metadata": {
        "id": "AUjoJyYJ5bTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning is field of study that system learns by itself from data without explicitly programming it.\n",
        "> Machine learning divided into two main categories\n",
        "  1. Supervised Learning\n",
        "  2. Unsupervised Learning\n",
        "  (Reinforcement Learnining)\n"
      ],
      "metadata": {
        "id": "QHR5SVHT5pPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Supervised Learning\n",
        "```\n",
        "Supervised Learning is the type where ML algorithms learn with the answer\n",
        "1.x refers to input data the features, y refers to the output category(answer)\n",
        "2.the ML model learns from the x(input features) and predict (ŷ => yhat)\n",
        "main types of Supervised Learning\n",
        "1.Classification\n",
        "2.Regression\n",
        "examples: classifying a mail into spam or ham categories, classifying an image into cat or dog,\n",
        "classifying the users based on info that they click advertisement or not... etc (classifying into a defined set of categories)\n",
        "Regression is another type of supervised learning where the model predicts a value(ŷ) after analyzing the input features(x).\n",
        "```"
      ],
      "metadata": {
        "id": "qUAH_CHdBUGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Unsupervised Learning\n",
        "```\n",
        "Unsupervised Learning is one of the ML Algo classifications, where the machine gets input data(x),\n",
        "but there is no output (y), there are no answers in it.\n",
        "The Algo analyze the data and find the pattern in the data given.\n",
        "1. one of the unsupervised algo is Clustering\n",
        "this algo analyze the given input data(x) and forms the differnt clusters formed based on their featues(input)\n",
        "(anamoly detection, dimentionality reduction are some other unsupervised algos)\n",
        "examples: clustering custumers into different categories of purchase they can make, based on the details of the customers,\n",
        "clustering the group of fruits into different clusters of each fruit.\n",
        "```"
      ],
      "metadata": {
        "id": "bM1R8eF8Dzui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Linear Regression\n",
        "```\n",
        "fitting line in the data distribution(single feature input), that line is fitted by the Algo learing from the input data and forming a function.\n",
        "that function(model) is used for prediction of the new values.\n",
        "\n",
        "example: a uni- variate Linear Regression(house price detection)\n",
        "single feature - square feet\n",
        "output - price of the house\n",
        "\n",
        "for that Linear Regression\n",
        "```\n",
        "function f${(w,b)}$(x) = w(x) + b\n"
      ],
      "metadata": {
        "id": "T7apsAuOHUnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cost Function\n",
        "```\n",
        "in case of linear Regression we try to fit a line with the training data given to the ML algo,\n",
        "that helps in the prediction of the results for the new inputs.\n",
        "\n",
        "the above defined linear function representing line has two parameters (w,b)\n",
        "this values (parameters/coefficients/weights) need to be chosen (adjusted)\n",
        "to have a line that fits better to the training data.\n",
        "```\n",
        "f${(w,b)}$ (x^i) = w(x^i) + b\n",
        "```\n",
        "the (ŷ) this is the prediction of the algo, y is the actual value, (y-(ŷ)) is the error,\n",
        "(y-(ŷ))^2 is the squared error,\n",
        "summation of all 'm' rows error is calculated, for not getting big values for large data, mean squared error is used\n",
        "```\n",
        "J(w,b) = 1/2m Σ(i=1 to m) (y-(ŷ))^2\n",
        "```\n",
        "This is called teh cost function. that calculates the error in the prediction of the model,\n",
        "this is mean squared error, this is simple and common of linear regression,\n",
        "there are multiple other that are used in other cases.\n",
        "```"
      ],
      "metadata": {
        "id": "eltrk7oKz0B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "for different w,b diffent lines are formed\n",
        "the cost function calculate sthe error rate (prediction vs the actual value difference)\n",
        "GOAL is to find the values of w and b that makes the J(w,b) cost function as less as possible,\n",
        "that means we want the model to predict more acurately.\n",
        "\"minimize (w,b) J(w,b)\"\n",
        "\n",
        "** if we not consider 'b' value for a moment and let the line move through the origin with w =0**\n",
        "for every value of the w (-ve,+ve) for the input value 'x' it forms a line, and for the cost function that line calulates a value (error value) in the graph.\n",
        "\n",
        "we need the parameter value (w) that results with low cost function, which means accurate prediction\n",
        "```\n",
        "\n",
        "## so How to find the value of w?\n",
        "the value of w is the one, that gives lowest cost function value.\n",
        "in general case the values of parameters (w,b) are the one that gives lowest cost function."
      ],
      "metadata": {
        "id": "vLdvl329zz8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for the linear regression with (w,b) parameters, the graph for the cost function will be in 3-D, it has similar shape of a bowl,\n",
        "this 3-D shapes can be visualized in 2-D with the help of contour plot\n",
        "its teh top view of the 3-D graph, it looks like the 3rd dimention is cut down horizantally in to pieces and stiched up."
      ],
      "metadata": {
        "id": "BLKqJB63zz48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "```\n",
        "a way to find those (w,b) parameters not manual looking for the right pair of values,\n",
        "can be achived with the help of this algorithm(gradient descent).\n",
        "this algo is used in neural networks too.(deep learning models)\n",
        "\n",
        "\"Gradient Descent is algo that minimizes any function, with any parameters, not just cost function.\"\n",
        "\n",
        "high level view of what gradient descent does:\n",
        "** there will be so many cost functions (J) what have more than one lowest points(not just bell shape every time) for neural networks**\n",
        "1.start of with some initial values its important where you start little off in the values we start can lead to diffent low points.\n",
        "(for linear regression 0 as the starting point is fine)\n",
        "2.the Gradient Descent from the start points finds the path near to it that leads it to a lowest point fast.\n",
        "3.by doing this gradient descent lead the ML algo to reach a low point.\n",
        "\n",
        "that lowest point is the one of the best parameter values ((w,b) in linear regression) that has lowest cost function (low error and high accuracy)\n",
        "```"
      ],
      "metadata": {
        "id": "KR5-NC71zz0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient Descent Mathematically\n",
        "```\n",
        "for parameter 'w'\n",
        "w = w - α(d/dw(J(w,b)))\n",
        "'α' is the learning rate the rate at which the Gradient descent take steps to reach the lowest point.\n",
        "value of alpha is between 0 and 1 (the low value makes smallerrr steps, high value makes larger steps ), finding teh correct value is important.\n",
        "\n",
        "similarly,  all the parameters are updated like this\n",
        "b = b - α(d/db(J(w,b)))\n",
        "\n",
        "this means that from the original (starting point) parameter value remove a smaller part (move to next low step) the d/dw(J(w,b)) decides direction to go and the α defines the size of the step in that direction.\n",
        "\n",
        "** It is efficient and the correct way to update all the parameters simultaneouly, with the same initial values\n",
        "and not using uodated values in the calulation of later parameters, this can be achieved by calculating the right hand part first and storing them in a\n",
        "temporary variable and upadating all the parameters from temporary variables to original variables at once**\n",
        "```"
      ],
      "metadata": {
        "id": "VAG_U5gl5t8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The derivative part of the Gradient Descent\n",
        "```\n",
        "the derivative part adss or substracts from the original parameter to get closer to the low point in cost function.\n",
        "\n",
        "the derivative of the cost function (lets consider uni-feature 'w'),in the cost function graph\n",
        "for a point w the derivative will be equal to the tangent at that point to the curve (calculating with tan theta will give the slope) if slope is +ve number\n",
        "the new 'w' value will be less than the original and if slope is -ve the new 'w' value will be higher,\n",
        "although both take the similar path to reach the lowest points.\n",
        "```"
      ],
      "metadata": {
        "id": "q0VJSn6s5K4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Rate\n",
        "for wrong values of alpha the algo doesnt work as expected.\n",
        "1. a small value will make baby steps towars the minimum point thats too late and inefficient\n",
        "2. a large value has a risk of never reaching the minimum point, the larger steps might lead to missing the minimum point\n",
        "and never converge but may even diverge to the higher points\n",
        "\n",
        "** as the 'w' value gets closer to the minimum point the Gradient Descent will take more smaller steps as the slope value will bw less near the minimum point in the curve**"
      ],
      "metadata": {
        "id": "q5Ix5na05dh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**if the 'w' value is already at a local minima, the Gradient Descent can't move it from there due to slope at the point being zero that means the derivative is zero.\n",
        "there can be a lot of local minima points in the graph of cost function, there are also Global minima that is the lowest point of all**"
      ],
      "metadata": {
        "id": "cE2Oc226igC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent fot Linear Regression\n",
        "```\n",
        "for the linear regressioin the cost function wil be in a shape of a bowl having only one minimum point,\n",
        "so its easier in linear regression to implement the Gradient descent and reach the global minima.\n",
        "\n",
        "this Gradient Descent is called as BATCH GRADIENT DESCENT that for every step it sees all the input data\n",
        "(in the cost function all the data of 'm' rows is included in making step)\n",
        "```"
      ],
      "metadata": {
        "id": "ecTwQiROhW7g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_X24CUN3lW-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}