<!-- @format -->

# ML Progress

## Day 1: NumPy Fundamentals

**Date**: 18 sep  
**File**: `1Numpy.ipynb`

### What I Learned Today:

- **NumPy Basics**: Array creation, properties (shape, dtype, size), and indexing
- **Array Operations**: Slicing, boolean masking, and fancy indexing
- **Mathematical Operations**: Element wise operations, linear algebra (matrix multiplication, determinants)
- **Statistical Functions**: min, max, sum with axis operations
- **Array Manipulation**: Reshaping, stacking, copying vs references

### Key Functions Practiced:

np.array(), np.zeros(), np.ones(), np.random.randint()
np.matmul(), np.linalg.det(), np.vstack(), np.hstack()
.reshape(), .copy(), boolean operations (&, |, ~)

---

## Kaggle Learn – Intro to Machine Learning

**File**: `Intro to Machine Learning Kaggle`

I completed the entire [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) course on Kaggle.  
Here are the lessons I worked through (with links to my notebooks):

1. How Models Work
2. [Basic Data Exploration](https://www.kaggle.com/code/anirudh4v/exercise-explore-your-data)
3. [Your First Machine Learning Model](https://www.kaggle.com/code/anirudh4v/exercise-your-first-machine-learning-model)
4. [Model Validation](https://www.kaggle.com/code/anirudh4v/exercise-model-validation)
5. [Underfitting and Overfitting](https://www.kaggle.com/code/anirudh4v/exercise-underfitting-and-overfitting)
6. [Random Forests](https://www.kaggle.com/code/anirudh4v/exercise-random-forests)
7. [Machine Learning Competitions](https://www.kaggle.com/code/anirudh4v/exercise-machine-learning-competitions)

---

## Day 2: Pandas Fundamentals

**Date**: 19 Sep  
**File**: `2Pandas.ipynb`

### What I Learned Today:

- **Pandas Basics**: Series, DataFrame creation
- **Indexing & Selection**: `.loc[]`, `.iloc[]`, conditional filtering
- **Descriptive Statistics**: `.describe()`, `.info()`, `.value_counts()`
- **Data Cleaning**: handling NaN values, renaming columns, dropping rows/columns
- **Data Manipulation**: groupby, aggregation, sorting, merging/joining DataFrames

### Key Functions Practiced:

pd.Series(), pd.DataFrame(), df.loc[], df.iloc[]  
df.describe(), df.info(), df.groupby(), df.sort_values(), df.merge()

---

## Day 3: Kaggle Learn – Intermediate Machine Learning

**Date**: 20 Sep  
**File**: `Intermediate ML Kaggle`

I completed the entire [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) course on Kaggle.  
Here are the lessons I worked through (with links to my notebooks):

1. [Introduction](https://www.kaggle.com/code/anirudh4v/exercise-introduction)
2. [Missing Values](https://www.kaggle.com/code/anirudh4v/exercise-missing-values)
3. [Categorical Variables](https://www.kaggle.com/code/anirudh4v/exercise-categorical-variables)
4. [Pipelines](https://www.kaggle.com/code/anirudh4v/exercise-pipelines)
5. [Cross-Validation](https://www.kaggle.com/code/anirudh4v/exercise-cross-validation)
6. [XGBoost](https://www.kaggle.com/code/anirudh4v/exercise-xgboost)
7. [Data Leakage](https://www.kaggle.com/code/anirudh4v/exercise-data-leakage)

### Key Concepts Learned:

- Handling **missing values** effectively (dropping, imputation strategies)
- Working with **categorical variables** (ordinal encoding, one-hot encoding)
- Building and managing **pipelines** for clean workflows
- Using **cross-validation** to evaluate model performance reliably
- Implementing **XGBoost**, a powerful gradient boosting algorithm
- Identifying and preventing **data leakage** in ML projects

---

## Day 4: Coursera – Supervised Machine Learning: Regression and Classification

**Date**: 21 Sep  
**File**: `4ML_course1_Week1.ipynb`

I enrolled in the **Machine Learning Specialization by Coursera (DeepLearning.AI & Stanford)** today and
completed **Week 1** of the first course: _Supervised Machine Learning: Regression and Classification_.

### Week 1: Introduction to Machine Learning

#### Topics Covered:

- **Welcome & Applications of ML**
  - What ML is and where it’s applied in the real world
- **Core ML Concepts**
  - What is machine learning?
  - Supervised learning
  - Unsupervised learning
- **Tools for ML**
  - Introduction to Jupyter Notebooks
- **Regression**
  - Linear regression model
- **Cost Function**
  - Formula and intuition
  - Visualizing the cost function
  - Visualization examples
- **Gradient Descent**
  - Concept and implementation
  - Gradient descent intuition
  - Learning rate
  - Gradient descent for linear regression

---

### Notes:

This repository now contains my **NumPy** and **Pandas practice notebooks**, along with my completed **Kaggle ML courses**,
cousera **specialization in Machine Learning** documenting my daily ML learning journey.
